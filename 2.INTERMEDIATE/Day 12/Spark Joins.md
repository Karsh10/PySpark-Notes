### 1.What is a broadcast join?

A join where Spark sends the small DataFrame to every executor to eliminate shuffle.
### 2.**Driver**
 _The brain of the Spark job._

- Runs your program (PySpark code / SQL)
- Converts operations into a Directed Acyclic Graph (DAG)
- Schedules tasks on executors
- Holds metadata, schema info, job progress
**If driver dies â†’ whole application dies.**
### **3.Executors**

_Workers that actually do the work._

Executors:

- Run tasks
- Hold in-memory cache
- Do filtering, joins, aggregations, writes
- Produce output data
- Each executor has **RAM + CPU cores**
**Executors = workhorses**  
**Driver = manager**

### **4.Partitions**

_Chunks of data_
Each partition is processed by one task on one executor core.
- More partitions = more parallel processing
- Too many partitions = overhead
- Too few partitions = slow (underutilized cluster)
- 
Ideal partition size: **128MB**

### **5.Shuffle**

_The most expensive operation in Spark._

Spark moves data across executors:

- During join
- During groupBy
- During repartition

Shuffle causes:

- Disk writes
- Network transfer
- New stage creation

**Shuffle = slow  
Avoid as much as possible.**

---

### **6.Shuffle Partitions**

Default = **200 partitions**
Controls how many output partitions Spark will create during shuffle operations.
Set manually:
`spark.conf.set("spark.sql.shuffle.partitions", 50)`

# Step 1 ) Build Two Data Frames (Example)

(A big one + a small lookup table)

### ðŸ”¹ Big Table (Left-side)

```python
df_transactions = spark.createDataFrame
([     (1, "US", 100),     (2, "IN", 200),     (3, "UK", 150),     (4, "US", 80), ], ["id", "country_code", "amount"])
df_transactions.display()
```

### ðŸ”¹ Small Table (Right-side)

```python
df_countries = spark.createDataFrame
([     ("US", "United States"),     ("IN", "India"),     ("UK", "United Kingdom"), ], ["country_code", "country_name"])  
df_countries.display()
```

### âœ” Why two tables?

A **small table** should be **broadcasted** to speed up joins.
# Normal Join (SORT MERGE JOINS) (Slow)

If you just do:

```python
df_join = df_transactions.join(df_countries,df_transactions['country_code'] == df_countries['country_code'],"inner" ) 
df_join.display()
```

âž¡ Spark will use **SortMergeJoin**  
âž¡ This causes **shuffle on both DataFrames**  
âž¡ Expensive operation
# BROADCAST JOIN (FAST )
```python

df_join_opt = df_transactions.join(broadcast(df_countries),     df_transactions["country_code"] == df_countries["country_code"],"inner" ) df_join_opt.display()
```

### âœ” Why is this FAST?

- Spark copies **small table (countries)** to ALL executors
- No shuffle
- No sorting
- No expensive network transfer
- Lightning fast

# SQL HINTS 

You can force SparkSQL to broadcast a table EVEN without using the PySpark API.

`df_transactions.createOrReplaceTempView("transactions") df_countries.createOrReplaceTempView("countries")`

### SQL Broadcast Hint:
```python

df_sql_opt = spark.sql("""SELECT /*+ BROADCAST(c) */*      
FROM transactions t      
JOIN countries c      
ON t.country_code = c.country_code """) 
df_sql_opt.display()
```

âœ” `/*+ BROADCAST(c) */` = hint for Catalyst Optimizer  
âœ” Applies only to the `c` table  
âœ” Forces Spark to use BroadcastHashJoin

# Why Broadcast Join Works Best

### Use broadcast join when:

- One table is **small** (typically < 10MB)
- One table is **lookup data**
- Country codes, category maps, product metadata, etc.

### Benefits:

- No shuffle
- No sort
- 10â€“50x faster than SortMergeJoin

---

# Internals (Simple Explanation)

### Without broadcast:

- Spark shuffles BOTH DataFrames
- Sorts by join key
- Merges sorted partitions â†’ BIG COST

### With broadcast:

- df_countries is copied to all workers
- Every worker joins its part of df_transactions locally
- Zero shuffle
- Zero sort
- Pure local processing

---

# When Broadcast Fails?

Broadcast will NOT happen if:

- Table > broadcast threshold (default 10MB)
- Too many columns
- Too many rows